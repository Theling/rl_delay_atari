program: run_experiment_rl_delay.py
method: grid
metric:
  goal: maximize
  name: episodic_reward
parameters:
  seed:
    values: [1, 2, 3]
  total_timesteps:
    values: [2000000]
  deepmind_wrapper:
    values: [True]
  env_name:
    values: ['BoxingNoFrameskip-v4']
#    values: ['AssaultNoFrameskip-v4', 'BreakoutNoFrameskip-v4', 'AtlantisNoFrameskip-v4', 'StarGunnerNoFrameskip-v4', 'RoadRunnerNoFrameskip-v4',
#    'GopherNoFrameskip-v4', 'BoxingNoFrameskip-v4', 'RobotankNoFrameskip-v4', 'VideoPinballNoFrameskip-v4', 'DemonAttackNoFrameskip-v4',]
  agent_type:
    values: ['delayed', 'augmented', 'oblivious', 'rnn']
  delay_value:
    values:  [0, 5, 15, 25]
  train_freq:
    values: [4]
  exploration_initial_eps:
    values: [1.0]
  learning_rate:
    values: [0.0001]
  target_network_update_freq:
    values: [1000]
  exploration_final_eps:
    values: [0.001]
  buffer_size:
    values: [25000]
  prioritized_replay:
    values: [True]
  fixed_frame_skip:
    values: [True]
  clone_full_state:
    values: [False]
  load_pretrained_agent:
    values: [False]


